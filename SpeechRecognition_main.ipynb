{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Recognition using Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team members: Adrian LÃ¶wenstein, Kiran Bacsa, Manuel Vonlanthen<br>\n",
    "Date:         22.01.2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly describe what we wanted to do, i.e. classify 20 out of 30 words, the other 10 as unknown...<br>\n",
    "List the words...<br>\n",
    "Discuss where the data is from...<br>\n",
    "Mention the difference between the original kaggle competition to what whe did, e.g. ignore silence, not using thensor flow, only work with training data, to not be dependent on the kaggle competition...<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recompute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING** If you set recompute to True this will reextract all featrues and re-classify all audio file, which will take several days, so do not do it. It is here for completeness so you can see how the steps were done during th project. We've already computed these steps and saved the results into pickle files. Our entire used data, as well as the pickle files can be found on \"Link\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recompute = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction (Adrian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the data set....<br>\n",
    "Describe the entire pipeline from audio file to feature vector for one audio dile...<br>\n",
    "Shortly mentione other Features that were tried...<br>\n",
    "Set up python function in which the features of the entire training set could be extracted and put it into a if recompute is true... <br>\n",
    "Load the pickle with all features in them,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first approach was to naively reuse the code from assignment 3 : spectral graph theory. By combining all our of samples into\n",
    "a single graph and extracting the resulting graph laplacian, we hope to identify clusters which would correspond to the different words that need to be classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING** set indexes for features_og instead of taking entire dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best results were obtained using the cosine distance as our kernel metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and normalize mfccs\n",
    "features = pd.DataFrame(features_og['mfcc'])\n",
    "features -= features.mean(axis=0)\n",
    "features /= features.std(axis=0)\n",
    "\n",
    "# compute spatial distance\n",
    "distances = spatial.distance.squareform(spatial.distance.pdist(features,'cosine'))\n",
    "\n",
    "# generate weights with RBF kernel\n",
    "n=distances.shape[0]\n",
    "kernel_width = distances.mean()\n",
    "weights = np.exp(np.divide(-np.square(distances),kernel_width**2))\n",
    "np.fill_diagonal(weights,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatly, increasing sparsity has the effect of reducing classification accuracy. We therefore decided to remove k-NN\n",
    "sparsification all together and keep the sample graph as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute laplacian\n",
    "degrees = np.sum(weights,axis=0)\n",
    "laplacian = np.diag(degrees**-0.5) @ (np.diag(degrees) - weights) @ np.diag(degrees**-0.5)\n",
    "laplacian = sparse.csr_matrix(laplacian)\n",
    "plt.spy(laplacian.todense());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the eigenvectors of the Laplacian matrix. These eigenvectors will be used as feature vectors for our \n",
    "classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = sparse.linalg.eigsh(A=laplacian,k=25,which='SM')\n",
    "plt.plot(eigenvalues[1:], '.-', markersize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(5, 5, figsize=(17, 8))\n",
    "for i in range(1,6):\n",
    "    for j in range(1,6):\n",
    "        x = eigenvectors[:,i]\n",
    "        y = eigenvectors[:,j]\n",
    "        labels = np.sign(x)\n",
    "        axes[i-1,j-1].scatter(x, y, c=labels, cmap='RdBu', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on to the classification phase, we first need to split our samples into a training and a test set. In order to \n",
    "prevent our classifier from being biased by the training set, we made sure that both sets have an equal proportion of classes \n",
    "(i.e. words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample classes such as test and train have equal proportion of classes\n",
    "def split_sample_equal(label_vec, n_classes, train_size, test_size):\n",
    "\n",
    "    train_x = np.array([])\n",
    "    train_y = np.array([])\n",
    "\n",
    "    test_x = np.array([])\n",
    "    test_y = np.array([])\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        class_index = np.where(label_vec == (i+1))[0]\n",
    "        random_index = np.random.choice(range(len(class_index)), size=train_size+test_size, replace=False)\n",
    "\n",
    "        train_x_class = class_index[random_index[:train_size]]\n",
    "        train_y_class = label_vec[train_x_class]\n",
    "        train_x = np.append(train_x, train_x_class)\n",
    "        train_y = np.append(train_y, train_y_class)\n",
    "\n",
    "        test_x_class = class_index[random_index[train_size:train_size+test_size]]\n",
    "        test_y_class = label_vec[test_x_class]\n",
    "        test_x = np.append(test_x, test_x_class)\n",
    "        test_y = np.append(test_y, test_y_class)\n",
    "        \n",
    "    return train_x, train_y, test_x, test_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = split_sample_equal(label_vec, len(class_names), 80, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "train_features = eigenvectors[train_x.astype(int),:]\n",
    "test_features = eigenvectors[test_x.astype(int),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A wide range of classifiers were tested on our input features. Remarkably, a very simple classifier such as the [Gaussian Naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) produced far better results than more advanced techniques. This is mainly because the graph datapoints were generated using a gaussian kernel, and is therefore sensible to assume that our feature distribution will be gaussian as well. However, the best results were obtained using a [Quadratic Discriminant Analysis classifier](https://en.wikipedia.org/wiki/Quadratic_classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_test(clf, train_x, train_y, test_x, test_y):\n",
    "    clf.fit(train_x, train_y)  \n",
    "    predict_y = clf.predict(test_x)\n",
    "    print('accuracy : ', np.sum(test_y==predict_y)/len(test_y))\n",
    "    return predict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = QuadraticDiscriminantAnalysis()\n",
    "predict_y = fit_and_test(clf, train_features, train_y, test_features, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our test set has been classified we can visualize the effectiveness of our classification using a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(test_y, predict_y, class_names):\n",
    "\n",
    "    conf_mat=confusion_matrix(test_y,predict_y)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(conf_mat/np.sum(conf_mat,axis=1),cmap=plt.cm.hot)\n",
    "    tick = np.arange(len(class_names))\n",
    "    plt.xticks(tick, class_names,rotation=90)\n",
    "    plt.yticks(tick, class_names)\n",
    "    plt.ylabel('ground truth')\n",
    "    plt.xlabel('prediction')\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_y, predict_y, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can focus on the core words that need to be classified and label the rest as 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_adapted = np.sum(test_y_adapted==predict_y_adapted)/len(test_y_adapted)\n",
    "print('accuracy for main words classification : ', acc_adapted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_y_adapted,predict_y_adapted, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we can say that, using spectral clustering, we were able to leverage the properties of graph theory to find \n",
    "relevant features in speech recognition. However, the accuracy achieved with our model is too low for any practical applications. Moreover, this model does not benefit from sparsity, meaning that it will not be able to scale with large datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Supervised Classification (Manuel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I this section we describe the entire semi-supervised classification we used in detail. It is basically a step by step description\n",
    "of the semisup pipeline using only one test batch of 200 in a Graph of 5000 nodes. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Validation (Kiran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe that we tweaked parameters and which were found to be the optimal ones, comment on it...<br>\n",
    "Show the resulting accuracy that resulted when going trough the entire training set. For this, simply load a pickle or numpy array, with the results in it and comment on it<br>\n",
    "Add a .py funtion with which we could theoretically call to recompute the accuracy of the entire training set (in a \"if recompute is true\" conditioning). Add the function into main_pipeline.py. <br>\n",
    "(Optional) Also add clustering approach to compare, otherwise we will just mention it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = split_sample_equal(label_vec, len(class_names), 160, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess these will probably already be in Manuel's part..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "NEIGHBORS = 30\n",
    "n_batch = int(len(test_x) / batch_size)\n",
    "\n",
    "alpha = 1e-3\n",
    "beta = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tot = []\n",
    "accuracy = []\n",
    "remaining_test = np.array(test_x)\n",
    "\n",
    "# encode training samples classes into 1-hot array\n",
    "n_class = np.max(train_y)\n",
    "Y = np.eye(len(class_names))[train_y - 1].T\n",
    "\n",
    "for batch in tqdm(range(n_batch)):\n",
    "    potential_elements  = np.array(list(enumerate(remaining_test)))\n",
    "    indices = np.random.choice(potential_elements[:,0].reshape(-1,), batch_size, replace=False)\n",
    "    batch_index = potential_elements[:,0].reshape(-1,)[indices]\n",
    "    remaining_test = np.delete(remaining_test, indices)\n",
    "        \n",
    "    features = pd.DataFrame(features_og['mfcc'], np.append(train_x, batch_index))\n",
    "    features -= features.mean(axis=0)\n",
    "    features /= features.std(axis=0)\n",
    "\n",
    "    # keep mapping from batch id to main id\n",
    "    id_mapping = dict(zip(features.index, range(len(features))))\n",
    "    \n",
    "    distances = spatial.distance.squareform(spatial.distance.pdist(features,'cosine'))\n",
    "\n",
    "    n=distances.shape[0]\n",
    "    kernel_width = distances.mean()\n",
    "    weights = np.exp(np.divide(-np.square(distances),kernel_width**2))\n",
    "    np.fill_diagonal(weights,0)\n",
    "    \n",
    "    for i in range(weights.shape[0]):\n",
    "        idx = weights[i,:].argsort()[:-NEIGHBORS]\n",
    "        weights[i,idx] = 0\n",
    "        weights[idx,i] = 0\n",
    "        \n",
    "    degrees = np.sum(weights,axis=0)\n",
    "    laplacian = np.diag(degrees**-0.5) @ (np.diag(degrees) - weights) @ np.diag(degrees**-0.5)\n",
    "    laplacian = sparse.csr_matrix(laplacian)\n",
    "\n",
    "    # add test samples to 1-hot array\n",
    "    M = np.zeros((len(class_names), len(train_y) + batch_size)) # mask matrix\n",
    "    M[:len(train_y),:len(train_y)] = 1\n",
    "    Y_compr = np.concatenate((Y, np.zeros((len(class_names), batch_size))), axis=1)\n",
    "    y = np.concatenate((train_y,np.zeros((batch_size,))))\n",
    "\n",
    "    # Solve\n",
    "    X = solve(Y_compr, M, laplacian, alpha = alpha, beta = beta)\n",
    "\n",
    "    # Make label vector\n",
    "    x_hat = np.argmax(X,axis = 0)+np.ones(X[0,:].shape)\n",
    "\n",
    "    # Unify labels 13-30\n",
    "    x_hat_adapted = adapt_labels(x_hat)\n",
    "    true_y = np.concatenate((train_y,label_vec[batch_index]))\n",
    "    y_adapted = adapt_labels(true_y)\n",
    "    \n",
    "    # Only consider unknowns\n",
    "    accuracy_tot.append(np.sum(x_hat[(len(x_hat)-batch_size):]==true_y[(len(x_hat)-batch_size):])/batch_size) # for all 30 words\n",
    "    accuracy.append(np.sum(x_hat_adapted[(len(x_hat)-batch_size):]==y_adapted[(len(x_hat)-batch_size):])/batch_size) # only core words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(accuracy_tot)\n",
    "print(accuracy)\n",
    "print('The achieved overall accuracy was {}%'.format(round(np.sum(accuracy)/len(accuracy)*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
