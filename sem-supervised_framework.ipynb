{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Framework, adapted for sem-supervised learning\n",
    "This framework heavily borrows from the asic framework. It can be used to anaqlyze sem-supervised learning for the problem of speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# Math\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import librosa.display\n",
    "from scipy import sparse, stats, spatial\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython.display as ipd\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import pandas as pd\n",
    "import pygsp\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (17, 5)\n",
    "pygsp.plotting.BACKEND = 'matplotlib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the Data\n",
    "----\n",
    "Use `N` random samples for each word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "train_audio_path = '../Data/train/audio'\n",
    "dirs = [f for f in os.listdir(train_audio_path) if isdir(join(train_audio_path, f))]\n",
    "dirs.sort()\n",
    "\n",
    "path = []\n",
    "word = []\n",
    "speaker = []\n",
    "iteration = []\n",
    "\n",
    "for direct in dirs:\n",
    "    if not direct.startswith('_'):\n",
    "        # Random selection of N files per folder \n",
    "        list_files = os.listdir(join(train_audio_path, direct))\n",
    "        wave_selected  = list(np.random.choice([ f for f in list_files if f.endswith('.wav')],N,replace=False))\n",
    "        \n",
    "        # Extraction of file informations for dataframe\n",
    "        word.extend(list(np.repeat(direct,N,axis=0)))\n",
    "        speaker.extend([wave_selected[f].split('.')[0].split('_')[0] for f in range(N) ])\n",
    "        iteration.extend([wave_selected[f].split('.')[0].split('_')[-1] for f in range(N) ])\n",
    "        path.extend([train_audio_path + '/' + direct + '/' + wave_selected[f] for f in range(N)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the dataframe of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_og = pd.DataFrame({('info','word',''): word,\n",
    "                            ('info','speaker',''): speaker,\n",
    "                            ('info','iteration',''): iteration,\n",
    "                            ('info','path',''): path})\n",
    "index_og = [('info','word',''),('info','speaker',''),('info','iteration','')]\n",
    "#features_og.set_index(index_og,inplace=True)\n",
    "features_og.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Features Extraction\n",
    "----\n",
    "### 2.1 MFCC\n",
    "A classical but reliable set a features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_MFCC = 20\n",
    "\n",
    "def compute_mfcc(filepath):\n",
    "    audio, sampling_rate = librosa.load(filepath, sr=None, mono=True)\n",
    "    return librosa.feature.mfcc(y=audio,sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_name= ['mean','std','skew','kurtosis','median']\n",
    "col_names = [('mfcc',stat_name[i],j) for i in range(len(stat_name))  for j in range(N_MFCC)]\n",
    "features_mfcc =pd.DataFrame(columns=pd.MultiIndex.from_tuples(col_names),index=features_og.index)\n",
    "# sorting the columns in order to improve index performances (see lexsort errors)\n",
    "features_mfcc.sort_index(axis=1,inplace=True,sort_remaining=True)\n",
    "\n",
    "# MFCC FEATURES :\n",
    "for w in tqdm(range(len(features_og)),total=len(features_og),unit='waves'):\n",
    "    mfcc = compute_mfcc(features_og[('info','path')].iloc[w])\n",
    "    features_mfcc.loc[w, ('mfcc', 'mean')] = np.mean(mfcc,axis=1)\n",
    "    features_mfcc.loc[w, ('mfcc', 'std')] = np.std(mfcc,axis=1)\n",
    "    features_mfcc.loc[w, ('mfcc', 'skew')] = scipy.stats.skew(mfcc,axis=1)\n",
    "    features_mfcc.loc[w, ('mfcc', 'kurtosis')] = scipy.stats.kurtosis(mfcc,axis=1)\n",
    "    features_mfcc.loc[w, ('mfcc', 'median')] = np.median(mfcc,axis=1)\n",
    "    \n",
    "features_og = features_og.merge(features_mfcc,left_index=True,right_index=True)\n",
    "features_og.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the dataset features into a pickle to avoid to redo the computation on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_og.to_pickle('./Features Data/trainingFeatures.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set up graph using the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we take the features and build from it a weight matrix using the cosine distance (for now). We sparsify the weight matrix using the nearest neighbour method and finally make sure that the weight matrix is symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the pickle containing the previously saved features\n",
    "features_og = pd.read_pickle('./Features Data/trainingFeatures.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Features\n",
    "features = features_og['mfcc']\n",
    "features -= features.mean(axis=0)\n",
    "features /= features.std(axis=0)\n",
    "\n",
    "distances = spatial.distance.squareform(spatial.distance.pdist(features,'cosine'))\n",
    "\n",
    "n=distances.shape[0]\n",
    "kernel_width = distances.mean()\n",
    "weights = np.exp(np.divide(-np.square(distances),kernel_width**2))\n",
    "np.fill_diagonal(weights,0)\n",
    "\n",
    "# Show sthe weight matrix\n",
    "plt.matshow(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsify using nearest neighbours\n",
    "fix, axes = plt.subplots(2, 2, figsize=(17, 8))\n",
    "def plot(weights, axes):\n",
    "    axes[0].spy(weights)\n",
    "    axes[1].hist(weights[weights > 0].reshape(-1), bins=50);\n",
    "plot(weights, axes[:, 0])\n",
    "\n",
    "NEIGHBORS = 200\n",
    "\n",
    "for i in range(weights.shape[0]):\n",
    "    idx = weights[i,:].argsort()[:-NEIGHBORS]\n",
    "    weights[i,idx] = 0\n",
    "    weights[idx,i] = 0\n",
    "\n",
    "plot(weights, axes[:, 1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-supervised clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we perform semi-supervised clustering on the training set, i.e. we assume that we only know a percentage of the labels of the points and infere the other labels by optimizing a certain cost function. From here on we're working with teh package PyGSP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Graph using the weights matrix\n",
    "G = pygsp.graphs.Graph(weights)\n",
    "\n",
    "# Compute the normalized Graph Laplacian corresponding to the above constructed graph\n",
    "G.compute_laplacian('normalized')\n",
    "\n",
    "# Compute the Fourier basis of the Laplacian\n",
    "G.compute_fourier_basis(recompute=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use any two eigenvectors to plot the graph, I chose vecto 1 and two (not sure if it is the best)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Laplacian Eigenmaps to plot the graph in 2D\n",
    "G.set_coordinates(G.U[:,(1,2)])\n",
    "G.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a vector containing the label of each class that we have to classify (Leaving away silence for now). We then define the label vector as a graph signal and thus plot it on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Label vector\n",
    "# Define class name vector, the index will correspond to the calss label\n",
    "class_names = ['unknown','yes','no','up','down','left','right','on','off','stop','go']\n",
    "label_vec = np.ones(G.N)\n",
    "for i in range(0,len(class_names)):\n",
    "    label_vec +=(features_og['info','word'] == class_names[i]) *i\n",
    "\n",
    "G.plot_signal(label_vec, vertex_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two functions are used two randomly pick some percentage of samples form the label vector (prepare_observations) and two solve for the estimated vector (solve). The problem is set up in the exact same way as in assigment 4 (except we have more then two kind of labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_observations(p):\n",
    "    \"\"\"Prepare observations, where p is the percentage of values to keep.\"\"\"\n",
    "    rs = np.random.RandomState(42)\n",
    "    M = np.diag(rs.uniform(size=G.N) < p)\n",
    "    return M.dot(label_vec)\n",
    "\n",
    "def solve(y, alpha):\n",
    "    \"\"\"\n",
    "    Solve the optimization problem.\n",
    "    \n",
    "    Parameters:\n",
    "        y: the observations\n",
    "        alpha: the balance between fidelity and smoothness prior.\n",
    "    \n",
    "    Returns:\n",
    "        x_pred: the predicted class\n",
    "        x_star: the solution of the optimization problem\n",
    "    \"\"\"\n",
    "    M = np.diag(y!=0)\n",
    "    x_star = np.linalg.solve((M+alpha*G.L),y)\n",
    "    x_pred = np.round(x_star)\n",
    "\n",
    "    return x_pred, x_star\n",
    "\n",
    "# Play with the percentage of observed values.\n",
    "y = prepare_observations(p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the label_vector with only 50% of its labels\n",
    "G.plot_signal(y, vertex_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we compute and plot teh error rate of the estimation for compression rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p =[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "err = np.zeros(len(p))\n",
    "for i in range(len(p)):\n",
    "    y = prepare_observations(p[i])\n",
    "    x_pred, x_star = solve(y, alpha=1e-5)\n",
    "    err[i] = np.count_nonzero(label_vec-x_pred)/G.N \n",
    "    \n",
    "plt.plot(p,err)\n",
    "plt.title('Error vs. Compression Rate')\n",
    "plt.xlabel('Compression rate p')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion: \n",
    "Using the exact same model formulation as in assignment 4 and only one vector for the clustering seems not to work well. We have to adapth the model for our more complex case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
